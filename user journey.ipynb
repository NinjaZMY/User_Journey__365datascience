{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first cleaning function again, as the execution environment is reset between turns.\n",
    "import pandas as pd\n",
    "\n",
    "def remove_page_duplicates(data: pd.DataFrame, target_column: str = 'user_journey') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes sequences of sequentially repeating pages in the user journey strings.\n",
    "\n",
    "    Args:\n",
    "        data: The dataframe containing all the data.\n",
    "        target_column: The name of the column containing the user journey strings.\n",
    "\n",
    "    Returns:\n",
    "        A new dataframe with the cleaned-up journey strings.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    new_df = data.copy()\n",
    "\n",
    "    def clean_journey(journey: str) -> str:\n",
    "        if pd.isna(journey):\n",
    "            return journey\n",
    "\n",
    "        # Split the journey into pages\n",
    "        pages = journey.split('-')\n",
    "        \n",
    "        # Initialize a list for the cleaned journey\n",
    "        cleaned_pages = []\n",
    "        \n",
    "        # Iterate through the pages and keep only if it's different from the last added page\n",
    "        for page in pages:\n",
    "            if not cleaned_pages or page != cleaned_pages[-1]:\n",
    "                cleaned_pages.append(page)\n",
    "        \n",
    "        # Join the pages back into a single string\n",
    "        return '-'.join(cleaned_pages)\n",
    "\n",
    "    # Apply the cleaning function to the target column\n",
    "    new_df[target_column] = new_df[target_column].apply(clean_journey)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# --- Demo Data and Testing ---\n",
    "\n",
    "# Create demo data to test the function\n",
    "demo_data = pd.DataFrame({\n",
    "    'user_id': [1, 2, 3, 4],\n",
    "    'session_id': [101, 102, 103, 104],\n",
    "    'user_journey': [\n",
    "        \"Homepage-Pricing-Homepage\",              # Should remain unchanged\n",
    "        \"Homepage-Homepage-Homepage-Pricing\",     # Should become \"Homepage-Pricing\"\n",
    "        \"Checkout-Checkout-Review-Review-Final\",  # Should become \"Checkout-Review-Final\"\n",
    "        \"Login-Login-Homepage-Pricing-Login\"      # Should become \"Login-Homepage-Pricing-Login\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Original Demo Data:\")\n",
    "print(demo_data)\n",
    "# Original Demo Data:\n",
    "#    user_id  session_id                           user_journey\n",
    "# 0        1         101              Homepage-Pricing-Homepage\n",
    "# 1        2         102     Homepage-Homepage-Homepage-Pricing\n",
    "# 2        3         103  Checkout-Checkout-Review-Review-Final\n",
    "# 3        4         104     Login-Login-Homepage-Pricing-Login\n",
    "\n",
    "# Call the function on the demo data\n",
    "cleaned_demo_data = remove_page_duplicates(demo_data)\n",
    "\n",
    "print(\"\\nCleaned Demo Data:\")\n",
    "print(cleaned_demo_data)\n",
    "# Cleaned Demo Data:\n",
    "#    user_id  session_id                  user_journey\n",
    "# 0        1         101     Homepage-Pricing-Homepage\n",
    "# 1        2         102              Homepage-Pricing\n",
    "# 2        3         103         Checkout-Review-Final\n",
    "# 3        4         104  Login-Homepage-Pricing-Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4451809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define the first cleaning function\n",
    "def remove_page_duplicates(data: pd.DataFrame, target_column: str = 'user_journey') -> pd.DataFrame:\n",
    "    # ... (function definition is the same)\n",
    "    new_df = data.copy()\n",
    "    \n",
    "    def clean_journey(journey: str) -> str:\n",
    "        if pd.isna(journey):\n",
    "            return journey\n",
    "        pages = journey.split('-')\n",
    "        cleaned_pages = []\n",
    "        for page in pages:\n",
    "            if not cleaned_pages or page != cleaned_pages[-1]:\n",
    "                cleaned_pages.append(page)\n",
    "        return '-'.join(cleaned_pages)\n",
    "\n",
    "    new_df[target_column] = new_df[target_column].apply(clean_journey)\n",
    "    return new_df\n",
    "\n",
    "# Load the data from the CSV file\n",
    "df_raw = pd.read_csv(\"user_journey_raw.csv\")\n",
    "\n",
    "# Create a subset of the first 100 rows for demonstration\n",
    "df_raw_100 = df_raw.head(100)\n",
    "\n",
    "# Print the 100 rows of the RAW DataFrame\n",
    "print(\"--- 100 Rows of the RAW DataFrame ---\")\n",
    "print(df_raw_100.to_string())\n",
    "\n",
    "# Apply the cleaning function ONLY to the 100-row subset\n",
    "df_cleaned_100 = remove_page_duplicates(df_raw_100)\n",
    "\n",
    "# Print the 100 rows of the cleaned DataFrame\n",
    "print(\"\\n--- 100 Rows of the CLEANED DataFrame (only the subset was cleaned) ---\")\n",
    "print(df_cleaned_100.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d17d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Increase the maximum column width and set display options to ensure full string printing\n",
    "\"\"\" pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 10) \"\"\"\n",
    "\n",
    "# Load the data from the CSV file\n",
    "df_raw = pd.read_csv(\"user_journey_raw.csv\")\n",
    "\n",
    "def group_by(data: pd.DataFrame, group_column: str = 'user_id', target_column: str = 'user_journey', sessions = 'All', count_from: str = 'last') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Groups user journey strings for a given user ID, optionally restricting the number of sessions.\n",
    "    \n",
    "    *Fixes deprecation warning by using .head() and .tail() instead of .apply() for session slicing.*\n",
    "    \"\"\"\n",
    "    new_df = data.copy()\n",
    "\n",
    "    # 1. Sort by group_column and session_id to ensure chronological order within each user\n",
    "    if 'session_id' in new_df.columns:\n",
    "        new_df = new_df.sort_values(by=[group_column, 'session_id'])\n",
    "    else:\n",
    "        new_df = new_df.sort_values(by=[group_column])\n",
    "\n",
    "    # 2. Select the specified number of sessions using head/tail\n",
    "    if isinstance(sessions, int) and sessions > 0:\n",
    "        grouped = new_df.groupby(group_column, group_keys=False)\n",
    "\n",
    "        if count_from == 'first':\n",
    "            new_df = grouped.head(sessions)\n",
    "        elif count_from == 'last':\n",
    "            new_df = grouped.tail(sessions)\n",
    "\n",
    "    # 3. Group and aggregate the journey strings, joining them with a hyphen\n",
    "    grouped_df = new_df.groupby(group_column).agg(\n",
    "        {target_column: lambda x: '-'.join(x.astype(str))}\n",
    "    ).reset_index()\n",
    "\n",
    "    return grouped_df\n",
    "\n",
    "    # Print the original demo data (pre-grouped sessions)\n",
    "print(\"--- Original Demo Data (Sessions per User) ---\")\n",
    "print(df_raw[['user_id', 'session_id', 'user_journey']].to_string())\n",
    "# Output (partial, showing User ID 1516 and 3395 for context):\n",
    "#     user_id  session_id                                                  user_journey\n",
    "# 0      1516     2980231                                         Homepage-Log in-Other\n",
    "# 1      1516     2980248                                          Other-Sign up-Log in\n",
    "# 2      1516     2992252                                                        Log in\n",
    "# 3      1516     3070491                                               Homepage-Log in\n",
    "# 4      1516     3709807                                                        Log in\n",
    "# 5      1516     3723132                                                      Checkout\n",
    "# 6      1516     3723365                                                      Checkout\n",
    "# 7      1516     3723382                                                      Checkout\n",
    "# 8      1516     3723427                                                      Checkout\n",
    "# 9      1516     3723483                                                        Coupon\n",
    "# 10     1516     3723508                                                      Checkout\n",
    "# 11     1516     3724778                                                      Checkout\n",
    "# 12     1516     3726160                                                      Checkout\n",
    "# 13     3395     1415870                                                         Other\n",
    "# 14     3395     3645805                                        Pricing-Sign up-Log in\n",
    "# 15     3395     3657408                                              Homepage-Pricing\n",
    "# 16     3395     3712148                                              Pricing-Checkout\n",
    "# 17     3395     3713857                                                      Checkout\n",
    "# ... (and so on for other users)\n",
    "\n",
    "# Print the session count per test user BEFORE grouping\n",
    "print(\"\\n--- Session Count Per User (for test users) ---\")\n",
    "print(df_raw.groupby('user_id')['session_id'].count())\n",
    "# Output:\n",
    "# user_id\n",
    "# 1516     13\n",
    "# 3395      5\n",
    "# 10107    16\n",
    "# 11145    11\n",
    "# 12400     4\n",
    "# Name: session_id, dtype: int64\n",
    "\n",
    "\n",
    "# 1. Test 'All' sessions (Default)\n",
    "df_grouped_all = group_by(df_raw, sessions='All')\n",
    "print(\"\\n--- Test 1: Grouping 'All' Sessions \")\n",
    "print(df_grouped_all)\n",
    "# Output (User 1516):\n",
    "#    user_id                                                                                     user_journey\n",
    "# 0     1516  Homepage-Log in-Other-Other-Sign up-Log in-Log in-Homepage-Log in-Log in-Checkout-Checkout-Checkout-Checkout-Coupon-Checkout-Checkout-Checkout\n",
    "# ...\n",
    "\n",
    "# 2. Test grouping by the 'last' 3 sessions\n",
    "df_grouped_last_3 = group_by(df_raw, sessions=3, count_from='last')\n",
    "print(\"\\n--- Test 2: Grouping the 'Last' 3 Sessions \")\n",
    "print(df_grouped_last_3)\n",
    "# Output (User 1516):\n",
    "#    user_id                 user_journey\n",
    "# 0     1516  Checkout-Checkout-Checkout\n",
    "# ...\n",
    "\n",
    "# 3. Test grouping by the 'first' 2 sessions\n",
    "df_grouped_first_2 = group_by(df_raw, sessions=2, count_from='first')\n",
    "print(\"\\n--- Test 3: Grouping the 'First' 2 Sessions ---\")\n",
    "print(df_grouped_first_2)\n",
    "# Output (User 1516):\n",
    "#    user_id                    user_journey\n",
    "# 0     1516  Homepage-Log in-Other-Other-Sign up-Log in\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f5336",
   "metadata": {},
   "source": [
    "# task 1: Preprocessing the Data full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eec140eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final DataFrame Head (Confirming Column Order) ---\n",
      "   user_id subscription_type                                                                                                                                                                                                                     user_journey\n",
      "0     1516            Annual                                                                                                                                                    Homepage-Log in-Other-Sign up-Log in-Homepage-Log in-Checkout-Coupon-Checkout\n",
      "1     3395            Annual                                                                                                                                                                           Other-Pricing-Sign up-Log in-Homepage-Pricing-Checkout\n",
      "2    10107            Annual  Homepage-Career tracks-Homepage-Career tracks-Sign up-Log in-Homepage-Resources center-Other-Homepage-Career tracks-Courses-Career tracks-Courses-Career tracks-Log in-Homepage-Log in-Checkout-Log in-Checkout-Log in-Checkout\n",
      "3    11145           Monthly                                                                         Homepage-Log in-Homepage-Log in-Homepage-Log in-Homepage-Log in-Homepage-Log in-Homepage-Log in-Homepage-Log in-Homepage-Log in-Homepage-Log in-Checkout\n",
      "4    12400           Monthly                                                                                              Homepage-Career tracks-Sign up-Log in-Other-Career track certificate-Resources center-Homepage-Instructors-Homepage-Log in-Checkout\n",
      "\n",
      "--- Full Pipeline Reordered Execution Complete ---\n",
      "Pipeline Order: Function 2 (group_by) -> Function 3 (remove_pages) -> Function 1 (remove_page_duplicates)\n",
      "Final DataFrame has 1350 rows (one per unique user).\n",
      "Saved cleaned data to user_journey_cleaned_reordered.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set options to display full journey strings (optional, but good practice)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "# --- Function 1: Remove Sequential Page Duplicates ---\n",
    "def remove_page_duplicates(data: pd.DataFrame, target_column: str = 'user_journey') -> pd.DataFrame:\n",
    "    \"\"\"Removes sequences of sequentially repeating pages in the user journey strings.\"\"\"\n",
    "    new_df = data.copy()\n",
    "    def clean_journey(journey: str) -> str:\n",
    "        if pd.isna(journey):\n",
    "            return journey\n",
    "        pages = journey.split('-')\n",
    "        cleaned_pages = []\n",
    "        for page in pages:\n",
    "            if not cleaned_pages or page != cleaned_pages[-1]:\n",
    "                cleaned_pages.append(page)\n",
    "        return '-'.join(cleaned_pages)\n",
    "    new_df[target_column] = new_df[target_column].apply(clean_journey)\n",
    "    return new_df\n",
    "\n",
    "# --- Function 2: Group User Journeys by ID and Session Count (MODIFIED for column order) ---\n",
    "def group_by(data: pd.DataFrame, group_column: str = 'user_id', target_column: str = 'user_journey', sessions = 'All', count_from: str = 'last') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Groups user journey strings for a given user ID, optionally restricting the number of sessions.\n",
    "    Preserves 'subscription_type' and ensures it is the second column (index 1).\n",
    "    \"\"\"\n",
    "    new_df = data.copy()\n",
    "    if 'session_id' in new_df.columns:\n",
    "        new_df = new_df.sort_values(by=[group_column, 'session_id'])\n",
    "    else:\n",
    "        new_df = new_df.sort_values(by=[group_column])\n",
    "\n",
    "    if isinstance(sessions, int) and sessions > 0:\n",
    "        grouped = new_df.groupby(group_column, group_keys=False)\n",
    "        if count_from == 'first':\n",
    "            new_df = grouped.head(sessions)\n",
    "        elif count_from == 'last':\n",
    "            new_df = grouped.tail(sessions)\n",
    "\n",
    "    # MODIFICATION: Explicitly define the aggregation dictionary keys in the desired output order.\n",
    "    agg_dict = {}\n",
    "    if 'subscription_type' in new_df.columns:\n",
    "        # 1. Place 'subscription_type' first in the aggregation dict (it will be column 2 after user_id)\n",
    "        agg_dict['subscription_type'] = 'first'\n",
    "        \n",
    "    # 2. Place 'user_journey' second in the aggregation dict (it will be column 3)\n",
    "    agg_dict[target_column] = lambda x: '-'.join(x.astype(str))\n",
    "        \n",
    "    grouped_df = new_df.groupby(group_column).agg(agg_dict).reset_index()\n",
    "    \n",
    "    # Final check and explicit reordering of columns to ensure user_id, subscription_type, user_journey\n",
    "    # Although agg typically orders by dict keys, explicit selection is safest.\n",
    "    final_cols = [group_column]\n",
    "    if 'subscription_type' in grouped_df.columns:\n",
    "        final_cols.append('subscription_type')\n",
    "    final_cols.append(target_column)\n",
    "\n",
    "    # Return the DataFrame with the correct column order\n",
    "    return grouped_df[final_cols]\n",
    "\n",
    "# --- Function 3: Remove Specific Pages from Journey String (No Change) ---\n",
    "def remove_pages(data: pd.DataFrame, pages: list, target_column: str = 'user_journey') -> pd.DataFrame:\n",
    "    \"\"\"Removes specific pages from the user journey strings in the target column.\"\"\"\n",
    "    new_df = data.copy()\n",
    "    pages_to_remove = set(pages)\n",
    "    def filter_journey(journey: str) -> str:\n",
    "        if pd.isna(journey) or not journey:\n",
    "            return journey\n",
    "        filtered_pages = [page for page in journey.split('-') if page not in pages_to_remove]\n",
    "        return '-'.join(filtered_pages)\n",
    "    new_df[target_column] = new_df[target_column].apply(filter_journey)\n",
    "    return new_df\n",
    "\n",
    "# --- FULL PIPELINE EXECUTION (Order: F2 -> F3 -> F1) ---\n",
    "\n",
    "# Load the raw data\n",
    "df_raw = pd.read_csv(\"user_journey_raw.csv\")\n",
    "\n",
    "# Step 1 (Function 2): Group all sessions, ensuring 'subscription_type' is column 2\n",
    "df_step_2 = group_by(df_raw, sessions='All')\n",
    "\n",
    "# Step 2 (Function 3): Remove specific pages (none yet, as requested)\n",
    "pages_to_exclude = []\n",
    "df_step_3 = remove_pages(df_step_2, pages=pages_to_exclude)\n",
    "\n",
    "# Step 3 (Function 1): Remove sequential duplicates from the final long journey string.\n",
    "df_final = remove_page_duplicates(df_step_3)\n",
    "\n",
    "# 4. Export the final DataFrame to CSV\n",
    "output_filename = 'user_journey_cleaned_reordered.csv'\n",
    "df_final.to_csv(output_filename, index=False)\n",
    "\n",
    "# Inspect the final DataFrame to confirm 'subscription_type' is the second column\n",
    "print(f\"--- Final DataFrame Head (Confirming Column Order) ---\")\n",
    "print(df_final.head().to_string())\n",
    "\n",
    "print(f\"\\n--- Full Pipeline Reordered Execution Complete ---\")\n",
    "print(f\"Pipeline Order: Function 2 (group_by) -> Function 3 (remove_pages) -> Function 1 (remove_page_duplicates)\")\n",
    "print(f\"Final DataFrame has {len(df_final)} rows (one per unique user).\")\n",
    "print(f\"Saved cleaned data to {output_filename}\")\n",
    "\n",
    "# Reset options\n",
    "pd.reset_option('display.max_colwidth')\n",
    "pd.reset_option('display.width')\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323cb5e0",
   "metadata": {},
   "source": [
    "# task 2: Analyzing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cbe2a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loaded 1350 cleaned user journeys. ---\n",
      "--- Starting Analysis based on Subscription Plan ---\n",
      "\n",
      "================================================================================\n",
      "1. PAGE COUNT: Total Occurrences of Each Page\n",
      "\n",
      "Analysis for Plan: All\n",
      "    Page  Count\n",
      "Homepage   2679\n",
      "  Log in   2234\n",
      "Checkout   1351\n",
      " Sign up   1247\n",
      "   Other   1189\n",
      "\n",
      "Analysis for Plan: Annual\n",
      "    Page  Count\n",
      "Homepage   1751\n",
      "  Log in   1361\n",
      "   Other    899\n",
      "Checkout    770\n",
      " Sign up    721\n",
      "\n",
      "Analysis for Plan: Monthly\n",
      "    Page  Count\n",
      "Homepage    819\n",
      "  Log in    779\n",
      "Checkout    518\n",
      " Sign up    461\n",
      " Courses    354\n",
      "\n",
      "================================================================================\n",
      "2. PAGE PRESENCE: Number of Journeys that include the Page\n",
      "\n",
      "Analysis for Plan: All\n",
      "    Page  Presence Count\n",
      "Homepage             843\n",
      "Checkout             821\n",
      "  Log in             756\n",
      " Sign up             738\n",
      "   Other             623\n",
      "\n",
      "Analysis for Plan: Annual\n",
      "    Page  Presence Count\n",
      "  Coupon             557\n",
      "Homepage             533\n",
      "  Log in             465\n",
      "Checkout             450\n",
      "   Other             435\n",
      "\n",
      "Analysis for Plan: Monthly\n",
      "    Page  Presence Count\n",
      "Checkout             333\n",
      "Homepage             278\n",
      " Sign up             278\n",
      "  Log in             259\n",
      "   Other             172\n",
      "\n",
      "================================================================================\n",
      "3. PAGE DESTINATION: Most Frequent Page Transitions (Top 5)\n",
      "\n",
      "Analysis for Plan: All\n",
      "Source Page Target Page  Count\n",
      "   Homepage      Log in    953\n",
      "     Log in    Homepage    817\n",
      "     Log in    Checkout    701\n",
      "   Homepage     Pricing    449\n",
      "    Sign up      Log in    394\n",
      "\n",
      "Analysis for Plan: Annual\n",
      "     Source Page Target Page  Count\n",
      "        Homepage      Log in    584\n",
      "          Log in    Homepage    498\n",
      "          Log in    Checkout    365\n",
      "        Homepage     Pricing    329\n",
      "Resources center       Other    314\n",
      "\n",
      "Analysis for Plan: Monthly\n",
      "Source Page Target Page  Count\n",
      "   Homepage      Log in    336\n",
      "     Log in    Checkout    295\n",
      "     Log in    Homepage    289\n",
      "    Sign up      Log in    155\n",
      "   Homepage     Sign up    118\n",
      "\n",
      "================================================================================\n",
      "4. PAGE SEQUENCES (N=3): Most Popular Run of 3 Pages (Top 5)\n",
      "\n",
      "Analysis for Plan: All\n",
      "         3-Page Sequence  Count (Unique Journeys)\n",
      "Homepage-Log in-Checkout                      239\n",
      "  Log in-Homepage-Log in                      220\n",
      "Homepage-Log in-Homepage                      180\n",
      " Sign up-Log in-Checkout                      123\n",
      " Sign up-Log in-Homepage                      104\n",
      "\n",
      "Analysis for Plan: Annual\n",
      "          3-Page Sequence  Count (Unique Journeys)\n",
      " Homepage-Log in-Checkout                      129\n",
      "   Log in-Homepage-Log in                      121\n",
      " Homepage-Log in-Homepage                      102\n",
      "Homepage-Pricing-Checkout                       75\n",
      "Homepage-Sign up-Homepage                       58\n",
      "\n",
      "Analysis for Plan: Monthly\n",
      "         3-Page Sequence  Count (Unique Journeys)\n",
      "Homepage-Log in-Checkout                      101\n",
      "  Log in-Homepage-Log in                       91\n",
      "Homepage-Log in-Homepage                       71\n",
      " Sign up-Log in-Checkout                       53\n",
      " Sign up-Homepage-Log in                       47\n",
      "\n",
      "================================================================================\n",
      "5. JOURNEY LENGTH: Average, Min, and Max Length of Journeys\n",
      "   Plan  Total Journeys  Average Length (Pages)  Min Length (Pages)  Max Length (Pages)\n",
      "    All            1350                   10.44                   1                 361\n",
      " Annual             931                   10.08                   1                 361\n",
      "Monthly             376                   11.16                   1                 124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "# Set options to display full DataFrames and long strings\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', None) \n",
    "\n",
    "# --- Load the cleaned data ---\n",
    "# Note: Using the last generated file name which includes subscription_type and correct ordering.\n",
    "try:\n",
    "    df_cleaned = pd.read_csv(\"user_journey_cleaned_reordered.csv\")\n",
    "except FileNotFoundError:\n",
    "    # If the file is not found (e.g., in a different environment), we will try the reordered file\n",
    "    # or create a mock DataFrame for demonstration purposes.\n",
    "    print(\"Warning: 'user_journey_cleaned_ordered_final.csv' not found. Creating mock data.\")\n",
    "    df_cleaned = pd.DataFrame({\n",
    "        'user_id': [1, 2, 3, 4, 5],\n",
    "        'subscription_type': ['Annual', 'Monthly', 'Annual', 'Monthly', 'Annual'],\n",
    "        'user_journey': [\n",
    "            'Homepage-Pricing-Checkout',\n",
    "            'Homepage-Resources center-Homepage-Resources center-Sign up-Checkout',\n",
    "            'Homepage-Pricing-Pricing-Checkout',\n",
    "            'Homepage-Homepage-Log in-Log in-Checkout',\n",
    "            'Homepage-Sign up-Checkout'\n",
    "        ]\n",
    "    })\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 1. Page Count: Total frequency of each page in all filtered journeys\n",
    "# =================================================================\n",
    "def page_count(data: pd.DataFrame, plan: str = 'All') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Counts the total occurrences of each page in the user_journey column.\n",
    "    Filters the analysis by the specified subscription plan.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    if plan != 'All':\n",
    "        df = df[df['subscription_type'] == plan]\n",
    "\n",
    "    # Combine all journey strings into a single list of pages\n",
    "    all_pages = '-'.join(df['user_journey'].dropna().astype(str)).split('-')\n",
    "    \n",
    "    # Calculate frequencies\n",
    "    page_freq = Counter(all_pages)\n",
    "    \n",
    "    # Convert to DataFrame for a clean output\n",
    "    result_df = pd.DataFrame(page_freq.items(), columns=['Page', 'Count'])\n",
    "    result_df = result_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# =================================================================\n",
    "# 2. Page Presence: Count of journeys that contain each page (max once per journey)\n",
    "# =================================================================\n",
    "def page_presence(data: pd.DataFrame, plan: str = 'All') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Counts the number of user journeys that contain each unique page (max once per journey).\n",
    "    Filters the analysis by the specified subscription plan.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    if plan != 'All':\n",
    "        df = df[df['subscription_type'] == plan]\n",
    "\n",
    "    presence_counts = Counter()\n",
    "\n",
    "    for journey in df['user_journey'].dropna().astype(str):\n",
    "        # Find unique pages in the current journey\n",
    "        unique_pages = set(journey.split('-'))\n",
    "        # Increment the counter for each unique page found\n",
    "        presence_counts.update(unique_pages)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame(presence_counts.items(), columns=['Page', 'Presence Count'])\n",
    "    result_df = result_df.sort_values(by='Presence Count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# =================================================================\n",
    "# 3. Page Destination: Most frequent follow-up pages\n",
    "# =================================================================\n",
    "def page_destination(data: pd.DataFrame, plan: str = 'All') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the most frequent immediate follow-up pages for every page.\n",
    "    Filters the analysis by the specified subscription plan.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    if plan != 'All':\n",
    "        df = df[df['subscription_type'] == plan]\n",
    "\n",
    "    transitions = Counter()\n",
    "\n",
    "    for journey in df['user_journey'].dropna().astype(str):\n",
    "        pages = journey.split('-')\n",
    "        \n",
    "        # Iterate over all transitions (Page X -> Page Y)\n",
    "        for i in range(len(pages) - 1):\n",
    "            source = pages[i]\n",
    "            target = pages[i+1]\n",
    "            transitions[(source, target)] += 1\n",
    "            \n",
    "    # Convert to DataFrame\n",
    "    if not transitions:\n",
    "        return pd.DataFrame(columns=['Source Page', 'Target Page', 'Count'])\n",
    "        \n",
    "    result_df = pd.DataFrame([\n",
    "        {'Source Page': source, 'Target Page': target, 'Count': count}\n",
    "        for (source, target), count in transitions.items()\n",
    "    ])\n",
    "    \n",
    "    result_df = result_df.sort_values(by='Count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# =================================================================\n",
    "# 4. Page Sequences: Most popular run of N pages (unique per journey)\n",
    "# =================================================================\n",
    "def page_sequences(data: pd.DataFrame, N: int, plan: str = 'All') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finds the most popular sequences of N pages. Counts each sequence only once per journey.\n",
    "    Filters the analysis by the specified subscription plan.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    if plan != 'All':\n",
    "        df = df[df['subscription_type'] == plan]\n",
    "\n",
    "    sequence_counts = Counter()\n",
    "\n",
    "    for journey in df['user_journey'].dropna().astype(str):\n",
    "        pages = journey.split('-')\n",
    "        sequences_in_journey = set()\n",
    "        \n",
    "        # Extract all N-page sequences\n",
    "        for i in range(len(pages) - N + 1):\n",
    "            sequence = '-'.join(pages[i:i + N])\n",
    "            sequences_in_journey.add(sequence)\n",
    "            \n",
    "        # Count each unique sequence found in this journey\n",
    "        sequence_counts.update(sequences_in_journey)\n",
    "            \n",
    "    # Convert to DataFrame\n",
    "    if not sequence_counts:\n",
    "        return pd.DataFrame(columns=[f'{N}-Page Sequence', 'Count (Unique Journeys)'])\n",
    "        \n",
    "    result_df = pd.DataFrame(sequence_counts.items(), columns=[f'{N}-Page Sequence', 'Count (Unique Journeys)'])\n",
    "    result_df = result_df.sort_values(by='Count (Unique Journeys)', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# =================================================================\n",
    "# 5. Journey Length: Average length of a user journey in pages\n",
    "# =================================================================\n",
    "def journey_length(data: pd.DataFrame, plan: str = 'All') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the average, min, and max length of a user journey in terms of pages.\n",
    "    Filters the analysis by the specified subscription plan.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    if plan != 'All':\n",
    "        df = df[df['subscription_type'] == plan]\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame({\n",
    "            'Plan': [plan],\n",
    "            'Average Length': [0],\n",
    "            'Min Length': [0],\n",
    "            'Max Length': [0],\n",
    "            'Total Journeys': [0]\n",
    "        })\n",
    "\n",
    "    # Calculate the length of each journey string\n",
    "    df['Journey_Length'] = df['user_journey'].apply(\n",
    "        lambda x: len(str(x).split('-')) if pd.notna(x) and x else 0\n",
    "    )\n",
    "\n",
    "    # Calculate statistics\n",
    "    avg_len = df['Journey_Length'].mean()\n",
    "    min_len = df['Journey_Length'].min()\n",
    "    max_len = df['Journey_Length'].max()\n",
    "    total_journeys = len(df)\n",
    "\n",
    "    # Convert to DataFrame for clean output\n",
    "    result_df = pd.DataFrame({\n",
    "        'Plan': [plan],\n",
    "        'Total Journeys': [total_journeys],\n",
    "        'Average Length (Pages)': [round(avg_len, 2)],\n",
    "        'Min Length (Pages)': [min_len],\n",
    "        'Max Length (Pages)': [max_len]\n",
    "    })\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# --- DEMONSTRATE ALL FIVE METRICS ---\n",
    "# =================================================================\n",
    "\n",
    "plans = ['All', 'Annual', 'Monthly']\n",
    "print(f\"--- Loaded {len(df_cleaned)} cleaned user journeys. ---\")\n",
    "print(f\"--- Starting Analysis based on Subscription Plan ---\")\n",
    "\n",
    "# 1. PAGE COUNT Demonstration\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. PAGE COUNT: Total Occurrences of Each Page\")\n",
    "for plan in plans:\n",
    "    print(f\"\\nAnalysis for Plan: {plan}\")\n",
    "    df_count = page_count(df_cleaned, plan=plan)\n",
    "    print(df_count.head(5).to_string(index=False))\n",
    "\n",
    "# 2. PAGE PRESENCE Demonstration\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. PAGE PRESENCE: Number of Journeys that include the Page\")\n",
    "for plan in plans:\n",
    "    print(f\"\\nAnalysis for Plan: {plan}\")\n",
    "    df_presence = page_presence(df_cleaned, plan=plan)\n",
    "    print(df_presence.head(5).to_string(index=False))\n",
    "\n",
    "# 3. PAGE DESTINATION Demonstration\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. PAGE DESTINATION: Most Frequent Page Transitions (Top 5)\")\n",
    "for plan in plans:\n",
    "    print(f\"\\nAnalysis for Plan: {plan}\")\n",
    "    df_dest = page_destination(df_cleaned, plan=plan)\n",
    "    print(df_dest.head(5).to_string(index=False))\n",
    "    \n",
    "# 4. PAGE SEQUENCES Demonstration (N=3)\n",
    "N_seq = 3\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"4. PAGE SEQUENCES (N={N_seq}): Most Popular Run of {N_seq} Pages (Top 5)\")\n",
    "for plan in plans:\n",
    "    print(f\"\\nAnalysis for Plan: {plan}\")\n",
    "    df_seq = page_sequences(df_cleaned, N=N_seq, plan=plan)\n",
    "    print(df_seq.head(5).to_string(index=False))\n",
    "    \n",
    "# 5. JOURNEY LENGTH Demonstration\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. JOURNEY LENGTH: Average, Min, and Max Length of Journeys\")\n",
    "all_length_results = []\n",
    "for plan in plans:\n",
    "    all_length_results.append(journey_length(df_cleaned, plan=plan))\n",
    "\n",
    "df_length_comparison = pd.concat(all_length_results, ignore_index=True)\n",
    "print(df_length_comparison.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
